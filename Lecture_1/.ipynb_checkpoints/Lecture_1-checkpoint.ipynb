{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Lecture 1\n",
    "\n",
    "### 1.0 Setup:\n",
    "\n",
    "At the beginning of every Python script, necessary packages should be downloaded, installed, and imported. Excellent free Python resources are provided by e.g. [QuantEcon.org](https://quantecon.org/ \"QuantEcon.org\"). In case you are unfamiliar with certain toolboxes or Python syntax you should definitely consider giving it a look. Moreover, the Python community is quite big and answers to questions can usually be found after a quick Google search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_centered' from 'scipy.signal.signaltools' (C:\\BI-Apps\\Anaconda3\\lib\\site-packages\\scipy\\signal\\signaltools.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msm\u001b[39;00m\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfigure.figsize\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m8\u001b[39m]\n",
      "File \u001b[1;32mC:\\BI-Apps\\Anaconda3\\lib\\site-packages\\statsmodels\\api.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscrete\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscrete_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Poisson, Logit, Probit,\n\u001b[0;32m     21\u001b[0m                                       MNLogit, NegativeBinomial,\n\u001b[0;32m     22\u001b[0m                                       GeneralizedPoisson,\n\u001b[0;32m     23\u001b[0m                                       NegativeBinomialP)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscrete\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcount_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ZeroInflatedPoisson,\n\u001b[0;32m     25\u001b[0m                                    ZeroInflatedGeneralizedPoisson,\n\u001b[0;32m     26\u001b[0m                                    ZeroInflatedNegativeBinomialP)\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m tsa\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mduration\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msurvfunc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SurvfuncRight\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mduration\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhazard_regression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PHReg\n",
      "File \u001b[1;32mC:\\BI-Apps\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\api.py:31\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvector_ar\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvecm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VECM\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvector_ar\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvar_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVAR\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m filters\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tsatools\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsatools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (add_trend, detrend, lagmat, lagmat2ds, add_lag)\n",
      "File \u001b[1;32mC:\\BI-Apps\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\filters\\api.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhp_filter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hpfilter\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcf_filter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cffilter\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfiltertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m miso_lfilter, convolution_filter, recursive_filter\n",
      "File \u001b[1;32mC:\\BI-Apps\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\filters\\filtertools.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfftpack\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfft\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m signal\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignaltools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _centered \u001b[38;5;28;01mas\u001b[39;00m trim_centered\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_like, PandasWrapper\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pad_nans\u001b[39m(x, head\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_centered' from 'scipy.signal.signaltools' (C:\\BI-Apps\\Anaconda3\\lib\\site-packages\\scipy\\signal\\signaltools.py)"
     ]
    }
   ],
   "source": [
    "# import Python packages\n",
    "%matplotlib inline  \n",
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import scipy as sp \n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import statsmodels.api as sm\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [10,8]  # Set default figure size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Data & Data Cleaning:\n",
    "For this lecture I chose a COVID-19 example. Free COVID-19 data for Norway can for example be obtained from [covid19data.no](https://www.covid19data.no/index.html \"COVID Norway\"). The website offers healthcare data, data on mobility, and data on business compensations and unemployment benefits. You are of course free to choose your own example. \n",
    "\n",
    "Here, I downloaded data on infections, data on mobility based on APPLE mobility trends, as well as data on COVID-19 testing. Let's suppose that we are interested in finding out whether changing our behaviour by e.g. walking or taking the car instead of public transport can reduce the number of infections. (Please keep in mind, that this is a highly stylized example that should be taken with a grain of salt.) Thus we are interested in estimating the model:\n",
    "\n",
    "$$ infections_t = \\alpha + \\beta_1 \\cdot walking_t + \\beta_2 \\cdot transit_t + \\beta_3 \\cdot driving_t + \\sum_{i=4}^k\\beta_i \\cdot C_{i,t} + \\varepsilon_t$$\n",
    "\n",
    "\n",
    "Accordingly, we will regress infections data on the mobility trends data and some control variables. To control for more tests leading to more positive results and hence more infections, the control variables include the number of COVID-19 tests conducted. \n",
    "\n",
    "Let's first read-in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the four data sets\n",
    "infections = pd.read_csv('municipality.csv')\n",
    "mobilityAPPL = pd.read_csv('mobilityAPPL.csv')\n",
    "testing = pd.read_csv('national_tests.csv')\n",
    "\n",
    "# convert the date column to datetime format\n",
    "infections['date'] = infections['date'].astype('datetime64[ns]')\n",
    "mobilityAPPL['date'] = mobilityAPPL['date'].astype('datetime64[ns]')\n",
    "testing['date'] = testing['date'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the date column as the index variable\n",
    "infections = infections.set_index('date')\n",
    "mobilityAPPL = mobilityAPPL.set_index('date')\n",
    "testing = testing.set_index('date')\n",
    "\n",
    "# sort the testing data in ascending order (it is the other way round in the raw data)\n",
    "testing = testing.sort_values(by='date', ascending=True)\n",
    "\n",
    "# A useful method to get a quick look at a data frame\n",
    "infections.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a quick look at the other dataframes (omitted here) we realise that the datasets are not aligned. We hence need to adjust the samples. Moreover, since the testing data is only available at national level, we have to aggregate the data across municipalities. We hence have to sum up all the cases that were recorded in different municipalities for every day. Moreover, to get rid of nonlinear trends in the data we will also take log-differences of the individual series (the logarithm \"linearises\" non-linear trends. Differencing then simply substracts these from the data). Note that you will learn about stationarity of time-series data in a later lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allign the sample length\n",
    "infections = infections['2020-04-01':'2020-12-22']\n",
    "mobilityAPPL = mobilityAPPL['2020-04-01':'2020-12-22']\n",
    "testing = testing['2020-04-01':'2020-12-22']\n",
    "\n",
    "# aggregate the daily infections data across municipalities. Parsing the argument as a list \"[['cases']]\" \n",
    "# automatically results in a new pd.DataFrame. Here we simply sum up the cases for every day.\n",
    "infections = infections.groupby('date')[['cases']].sum()\n",
    "\n",
    "# take log-differences of the cases\n",
    "infections['ld_cases'] = np.log(infections.cases) - np.log(infections.cases.shift(1))\n",
    "\n",
    "# take log-differences of the testing data. Since there is days with zero tests conducted,\n",
    "# we add a small epsilon to avoid taking logs of 0! \n",
    "testing['ld_ntests'] = np.log(testing.n_tests + 10**-13 ) - np.log(testing.n_tests.shift(1) + 10**-13)\n",
    "\n",
    "# mobility APPL already provides aggregated data. \n",
    "mobilityAPPL = mobilityAPPL[mobilityAPPL['region']=='Norway']\n",
    "\n",
    "# extract the mobility trends by type and rename the value column. Take log-differences\n",
    "driving = mobilityAPPL.loc[mobilityAPPL['transportation_type'] == 'driving', ['val']]\n",
    "driving.rename(columns = {'val':'ld_driving'}, inplace = True)\n",
    "driving.ld_driving =  np.log(driving.ld_driving) - np.log(driving.ld_driving.shift(1))\n",
    "\n",
    "transit = mobilityAPPL.loc[mobilityAPPL['transportation_type'] == 'transit', ['val']]\n",
    "transit.rename(columns = {'val':'ld_transit'}, inplace = True)\n",
    "transit.ld_transit = np.log(transit.ld_transit) - np.log(transit.ld_transit.shift(1))\n",
    "\n",
    "walking = mobilityAPPL.loc[mobilityAPPL['transportation_type'] == 'walking', ['val']]\n",
    "walking.rename(columns = {'val':'ld_walking'}, inplace = True)\n",
    "walking.ld_walking = np.log(walking.ld_walking) - np.log(walking.ld_walking.shift(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect the data into one dataframe. In order to do so, we use panda's \"merge\" function, where we merge using the date column. Once the merging is completed, we can drop all rows that contain missing values in one of the variables. Also let's add 10 lags of the dependent variable, which corresponds to the length of quarantine in Norway. The idea is that after 10 days patients that are infected will usually have developed symptoms and hence got tested. Hence, if more people are infected now, this should have a positive effect on infections over the next 10 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the list of dataframes to be merged\n",
    "dfs = [infections, driving, transit, walking, testing[['ld_ntests']]] \n",
    "\n",
    "# merge the datasets specified in 'dfs'. Here we use and one-line lambda function\n",
    "df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['date'], how='outer'), dfs)\n",
    "\n",
    "# drop rows that contain missing values\n",
    "df_merged = df_merged.dropna()\n",
    "\n",
    "# add a lagged dependent variables\n",
    "df_merged['ld_cases_1'] = df_merged.ld_cases.shift(1)\n",
    "df_merged['ld_cases_2'] = df_merged.ld_cases.shift(2)\n",
    "df_merged['ld_cases_3'] = df_merged.ld_cases.shift(3)\n",
    "df_merged['ld_cases_4'] = df_merged.ld_cases.shift(4)\n",
    "df_merged['ld_cases_5'] = df_merged.ld_cases.shift(5)\n",
    "df_merged['ld_cases_6'] = df_merged.ld_cases.shift(6)\n",
    "df_merged['ld_cases_7'] = df_merged.ld_cases.shift(7)\n",
    "df_merged['ld_cases_8'] = df_merged.ld_cases.shift(8)\n",
    "df_merged['ld_cases_9'] = df_merged.ld_cases.shift(9)\n",
    "df_merged['ld_cases_10'] = df_merged.ld_cases.shift(10)\n",
    "\n",
    "df_merged = df_merged.dropna()\n",
    "df_merged = df_merged[['ld_cases', 'ld_cases_1', 'ld_cases_2', 'ld_cases_3'\n",
    "                       , 'ld_cases_4', 'ld_cases_5', 'ld_cases_6', 'ld_cases_7'\n",
    "                       , 'ld_cases_8', 'ld_cases_9', 'ld_cases_10',\n",
    "                       'ld_driving', 'ld_transit', 'ld_walking', 'ld_ntests']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now successfully extracted the data we need and completed some initial data cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now have a brief look at the time series. In this context, plotting and eye-balling the data is always a good first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_merged[['ld_transit','ld_driving','ld_walking']].plot(lw=1)\n",
    "ax.set_xlabel('date', fontsize=12)\n",
    "plt.title(\"Mobility trends (log-differences)\")\n",
    "plt.show()\n",
    "\n",
    "ax2 = df_merged[['ld_cases','ld_ntests']].plot(lw=1)\n",
    "ax2.set_xlabel('date', fontsize=12)\n",
    "plt.title(\"Case & Test numbers (log-differences)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediately notice that the data seems to follow a strong seasonal pattern:\n",
    "\n",
    "* Testing might not be conducted on weekends \n",
    "* infections data might only be updated on weekdays\n",
    "* Compared to weekends, during the week people have to commute to work\n",
    "* people might take more walks during summer months compared to winter months\n",
    "* ...\n",
    "\n",
    "For this reason, we will also add weekday dummies and monthly/seasonal dummies to the regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------create weekday dummies-------------------------------------------------------------\n",
    "\n",
    "# specify weekdays \n",
    "names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# extract weekdays from dates\n",
    "df_merged['Weekday'] = df_merged.index.weekday\n",
    "\n",
    "# use pandas \"get_dummies\" to convert the weekday column into binary indicators/dummies that we join to the dataset\n",
    "df_merged = df_merged.join(pd.get_dummies(df_merged.Weekday))\n",
    "\n",
    "# apply the names to the newly created columns\n",
    "column_indices = list(range(16,23,1))\n",
    "old_names = df_merged.columns[column_indices]\n",
    "df_merged.rename(columns=dict(zip(old_names, names)), inplace=True)\n",
    "\n",
    "# drop the weekday column\n",
    "df_merged = df_merged.drop(['Weekday'], axis=1)\n",
    "\n",
    "# ---------------------------------------------------create monthly dummies-------------------------------------------------------------\n",
    "\n",
    "# specify months (we know that our data starts in April)\n",
    "names = ['Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "df_merged['month'] = df_merged.index.month\n",
    "\n",
    "# use pandas \"get_dummies\" to convert the month column into binary indicators/dummies that we join to the dataset\n",
    "df_merged = df_merged.join(pd.get_dummies(df_merged.month))\n",
    "\n",
    "# apply the names to the newly created columns\n",
    "column_indices = list(range(23,32,1))\n",
    "old_names = df_merged.columns[column_indices]\n",
    "df_merged.rename(columns=dict(zip(old_names, names)), inplace=True)\n",
    "\n",
    "# drop the month column. Also drop one dummy each due to multicoliniarity\n",
    "df_merged = df_merged.drop(['month','Monday','Apr'], axis=1)\n",
    "\n",
    "\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.0 Compute OLS regressions\n",
    " \n",
    " **You can ignore section 3.1 and jump to the implementation with `statsmodels` in section 3.2.**\n",
    " \n",
    " ### 3.1 Implementation with own Code\n",
    " \n",
    " \n",
    "After having compiled our data set, we are now ready to run a regression. As is specified above, we regress the infections data on our mobility trends data, an intercept, as well as the testing data and our dummies. Instead of the equation that we proposed originally, we will estimate the model:\n",
    "\n",
    "$$ \\Delta ln(infections_t) = \\alpha + \\beta_1 \\cdot \\Delta ln(walking_t) + \\beta_2 \\cdot \\Delta ln(transit_t) + \\beta_3 \\cdot \\Delta ln(driving_t) + \\sum_{i=4}^k\\beta_i \\cdot C_{i,t} + \\varepsilon_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the cases to a numpy array\n",
    "Y = df_merged.ld_cases.to_numpy()\n",
    "\n",
    "# convert numpy ndarray to array\n",
    "Y = np.asarray(Y)\n",
    "Y = Y.T\n",
    "\n",
    "# convert the rest of \n",
    "column_indices = list(range(1,29,1))\n",
    "X = df_merged[df_merged.columns[column_indices]].to_numpy()\n",
    "X = np.asarray(X)\n",
    "\n",
    "# add the intercept to the RHS/independent variables\n",
    "X = np.concatenate((np.ones([X.shape[0],1]), X), axis=1)\n",
    "\n",
    "varnames = np.asarray(df_merged.columns[column_indices])\n",
    "varnames = np.concatenate([['intercept'],varnames])\n",
    "\n",
    "# compute beta coefficients\n",
    "beta = np.linalg.inv(X.T @ X) @ (X.T @ Y)\n",
    "\n",
    "# compute the regression residuals\n",
    "eps = Y - X @ beta\n",
    "\n",
    "# compute the residual variance\n",
    "s_hat = 1/(len(Y)-len(beta)) * eps.T @ eps\n",
    "\n",
    "# compute the standard errors\n",
    "se = np.sqrt(np.diag(np.linalg.inv(X.T @ X) * s_hat.item()))\n",
    "\n",
    "# compute t-statistics\n",
    "t = np.abs(beta/se)\n",
    "\n",
    "# compute p-values using scipy's stats library\n",
    "p_vals = 2*(1-stats.norm.cdf(np.abs(t)))\n",
    "\n",
    "# compute confidence intervals\n",
    "CI_upper = beta + stats.norm.ppf(0.975)*se\n",
    "CI_lower = beta - stats.norm.ppf(0.975)*se\n",
    "\n",
    "# compute R^2\n",
    "MSE = np.mean(np.square(eps),axis=0)\n",
    "R2 = 1 - MSE/np.var(Y)\n",
    "\n",
    "# create output table\n",
    "outmat = np.stack((beta,se,t,p_vals,CI_lower,CI_upper),axis=1)\n",
    "table = pd.DataFrame(outmat,varnames)\n",
    "table.columns =['beta','se','t-statistic','p-value','CI - lower','CI - upper'] \n",
    "print('R^2:', R2.item())\n",
    "table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First notice, that the interpretation of our regression coefficients is somewhat tricky! Recall that for small changes log-differences can be interpreted as percentage changes (see an example for $x_{t}=12$ and $x_{t-1} = 11$):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Implementation with `statsmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the cases to a numpy array\n",
    "Y = df_merged.ld_cases.to_numpy()\n",
    "\n",
    "# convert numpy ndarray to array\n",
    "Y = np.asarray(Y)\n",
    "Y = Y.T\n",
    "\n",
    "# convert the rest of \n",
    "column_indices = list(range(1,29,1))\n",
    "X = df_merged[df_merged.columns[column_indices]].to_numpy()\n",
    "X = np.asarray(X)\n",
    "\n",
    "# add the intercept to the RHS/independent variables\n",
    "X = np.concatenate((np.ones([X.shape[0],1]), X), axis=1)\n",
    "\n",
    "# obtain lables for row names in output table\n",
    "varnames = np.asarray(df_merged.columns[column_indices])\n",
    "varnames = np.concatenate([['intercept'],varnames])\n",
    "\n",
    "# run the regression\n",
    "model = sm.OLS(Y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# print output table\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 4.1 General Interpretation\n",
    "Generally, the interpretation of the regression coefficient depends on the data transformation and the units of the input data:\n",
    "\n",
    "|Model      |Dependent Variable|Independent Variable|Interpretation        | Interpretation |\n",
    "|:-----      |:------------------:|:--------------------:|:--------------     | :-------------- |\n",
    "|level-level| $y$              |                $x$ | $\\Delta y = \\Delta x$     | A 1 unit increase in $x$, on average leads to a $\\beta$ unit increase in $y$, ceteris paribus.|\n",
    "|level-log  | $y$              | $log(x)$           | $\\Delta y=\\frac{\\beta}{100}\\% \\Delta x $ | A 1 percent increase in $x$, on average leads to a $\\beta/100$ unit change in $y$, ceteris paribus.|\n",
    "|log-level  | $log(y)$         | $x$                | $\\% \\Delta y= 100\\beta \\Delta x$ | A 1 unit increase in $x$, on average leads to a $100\\cdot\\beta$ percent change in $y$, ceteris paribus.|\n",
    "| log-log   | $log(y)$         | $log(x)$           | $\\% \\Delta y= \\beta \\% \\Delta x$ | A 1 percent increase in $x$, on average leads to a $\\beta$ percent change in $y$, ceteris paribus. |\n",
    "\n",
    "*Note that e.g. the interpretation of the coefficient in the log-level model is only approximately true. It is valid for small changes. *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we interpret **confidence intervals** as follows:\n",
    "* For a given value within the 95% confidence interval, we cannot reject the $H_0$ that the parameter estimate is equal to the given value, for any value within the confidence interval. \n",
    "\n",
    "\n",
    "**!!! Never say \"There is a 95% probability that the population parameter lies within the CI\" !!!**\n",
    "\n",
    "To interpret **p-value**, use:\n",
    "* Given the $H_0$ were true, the probability of observing a test statistic at least as large as the one obtained is \"p\" percent. \n",
    "\n",
    "**Also, never say \"I accept the H0\". Hypothesis can be rejected or not rejected but never accepted!**\n",
    "\n",
    "#### 4.2 Our Exercise\n",
    "\n",
    "Thus recall that in standard log-log regressions we interpret \"A $1$% increase in $X$, leads to a $\\beta$% change in $Y$\". We can now use a simple trick:\n",
    "\n",
    "For \n",
    "\n",
    "$$ ln(Y_t) = \\alpha + \\beta\\cdot ln(X_t) $$\n",
    "\n",
    "we have\n",
    "\n",
    "$$ ln(Y_t) - ln(Y_{t-1})= \\alpha + \\beta\\cdot ln(X_t) - \\alpha - \\beta\\cdot ln(X_{t-1}) $$\n",
    "\n",
    "or \n",
    "\n",
    "$$ \\Delta ln(Y_t)= \\beta\\cdot \\Delta ln(X_t) $$\n",
    "\n",
    "Notice that this is exactly the same $\\beta$! We can hence stick to the general interpretation.\n",
    "\n",
    "Let's look at \"driving\" as an example. A 1% increase in APPLE maps request for driving directions, leads to a -0.007% change in infections. An increase in APPLE maps requests for public transport and walking on the other hand, seems to have a positive effect on infections. Neither effect is statistically significant, however. \n",
    "\n",
    "\n",
    "Finally, notice that instead of computing the regressions manually, you can of course also use e.g. [`statsmodels.regression.linear_model.OLS`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html) or any other `Python library` that implements linear regression.\n",
    "\n",
    "\n",
    "### 5.0 Questions for Review:\n",
    "* How to interpret regression coefficients?\n",
    "* How to interpret confidence intervals?\n",
    "* How to interpret p-values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
